---
title: "Take Home Exercise 03"
description: |
  Building a hedonic pricing models to explain factors affecting the resale prices of Singapore public housing.
author:
  - name: Kwek Yi Chen
    url: https://kwekyichen-is415.netlify.app/posts/2021-10-27-takehome-ex03/
date: 10-27-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    number_sections: true
    code_folding: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## 1. Introduction

Housing is essential for everyone and is a major investment for many people in Singapore. Structural and location factor affects the price of housing. In this exercise, we are interested to find out which factor affects the price of 4-room HDB flat transacted from 1st January 2019 to 30th September 2020. We will be building a hedonic pricing model using GWR methods in the exercise.

## 2. Setting Up the Environment

We need to first set up the environment by install the relevant packages and set up the token.

### 2.1 Installing Packages

The following code chunk install the following packages:

- readr to read csv files
- tmap to plot chloropleth map
- sf to handle geospatial data
- onemapsgapi to query spatial data information from onemapsg
- httr to GET the response
- dplyr for attribute data handling
- stringr for simple wrappers for string
- jsonlite to convert r objects to/from JSON
- tidyr to tidy data
- sp to handle spatial data
- dummies: create dummy var
- geojsonio to convert various data format to/from GeoJSON
- ggplot2 to plot map
- ggpubr to arrange multiple map
- GWmodel for calibrating geographical weighted models
- corrplot to visualise and analyse multivariate data
- olsrr for building OLS regression models
- spdep for spatial dependence statistics

```{r}
packages = c('readr', 'tmap', 'sf', 'onemapsgapi', 'httr', 'dplyr', 'stringr', 'jsonlite', 'tidyr', 'sp', 'dummies', 'geojsonio', 'ggplot2', 'ggpubr', 'GWmodel', 'corrplot', 'olsrr', 'spdep')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
    }
  library(p,character.only = T)
}
```

### 2.2 onemapsgapi Token Authorization

Next, we set up the token for onemapsgapi. The code is not shown because the information is sensitive.

## 3. Data

- HDB Resale data is a list of hdb resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format from Data.gov.sg.
- Eldercare data is a list of eldercare in Singapore. It is in shapefile format from Data.gov.sg.
- Hawker Centre data is a list of hawker centres in Singapore. It is in geojson format from Data.gov.sg.
- MRT data is a list of MRT/LRT in Singapore. It is in shapefile format from Datamall.lta.gov.sg.
- Parks data is a list of parks in Singapore. It is in geojson format from Data.gov.sg. 
- Supermarket data is a list of supermarkets in Singapore. It is in geojson format from Data.gov.sg.
- CHAS clinic data is a list of CHAS clinics in Singapore. It is in geojson format from Data.gov.sg.
- Kindergartens data is a list of kindergartens in Singapore. It is in geojson format from Data.gov.sg.
- Childcare service data is a list of childcare services in Singapore. It is in geojson format from Data.gov.sg.
- Bus stop data is a list of bus stops in Singapore. It is in shapefile format from Datamall.lta.gov.sg.
- School data is a list of schools in Singapore. It is in csv format from Maply.com.

## 4. Data Preparation

We will be preparing the data in this section. Note that during the data preparation, files will be written in data/newaspatial as a backup. After data preparation, these backup files will be deleted deleted due to Github's file limit.

### 4.1 HDB Resales Data

The following code chunk uses read_csv function of readr package to import the hdb resales data and display the data.

```{r eval=FALSE}
hdbresale <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
hdbresale
```

The data contains multiple rows but our focus is only on four-room flat with transaction period from 1st January 2019 to 30th September 2020.

#### 4.1.1 Filter the relevant data

The following code chunk filters the four-room flat data from Jan 2019 to September 2020 using filter function of base package.

Then, we created a new column using mutate function of dplyr package for the following column:

- address column that combines the block and street name (paste function of base package concatenate the strings, as.character converts the string to character type)
- story_range_lower and storey_range_upper column that split the storey_range (str_sub function of stringr)
- remaining_lease_year column from the remaining_lease string (str_sub function of stringr)
- we selected all the columns
- display the data

```{r eval=FALSE}
hdbresale_filtered <- hdbresale %>%
                filter(month %in% c("2019-01", "2019-02", "2019-03", "2019-04", "2019-05", "2019-06", "2019-07", "2019-08", "2019-09", "2019-10", "2019-11", "2019-12","2020-01", "2020-02", "2020-03", "2020-04", "2020-05", "2020-06", "2020-07", "2020-08", "2020-09")) %>%
                filter(flat_type == "4 ROOM")

hdbresale_new <- hdbresale_filtered %>%
                  mutate(hdbresale_filtered, address = paste(as.character(block),as.character(street_name))) %>%
                  mutate(hdbresale_filtered, storey_range_lower = str_sub(storey_range, 0, 2)) %>%
                  mutate(hdbresale_filtered, storey_range_upper = str_sub(storey_range, -3, -1)) %>%
                  mutate(hdbresale_filtered, remaining_lease_year = str_sub(remaining_lease, 0, 2)) %>%
                  select(month, town, address, block, street_name, flat_type, storey_range, storey_range_lower, storey_range_upper, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, remaining_lease_year, resale_price)

hdbresale_new
```

After filtering, We can see that from Jan 2019 to September 2020, there are 15901 transactions for four-room flat in Singapore.

#### 4.1.2 Get lat long with address

Next, we get the get long with the address provided.

The follow code chunk:

- iterate through every rows of data in hdbresale_new table
- for each row, we will get the address and set the onemapsgapi url
- we will next get encode the url with URLencode function
- get the result of the encode_url with GET function of httr package
- extract the content from the getcoordinate request. as argument is set as "parsed"
- if there are at least one result, we extract the latitude and longitude and assign it to the LATITUDE and LONGITUDE column respectively

```{r eval=FALSE}
for (i in 1:nrow(hdbresale_new)) {
  address <- hdbresale_new[i,'address']
  
  url = paste('https://developers.onemap.sg/commonapi/search?searchVal=', address, '&returnGeom=Y&getAddrDetails=Y&pageNum=1')
  encoded_url <- URLencode(url)
  
  getcoordinate <- GET(encoded_url)
  
  jsonParsed <- content(getcoordinate,as="parsed")
  
  if (length(jsonParsed$results) > 0) {
    hdbresale_new[i,'LATITUDE'] = jsonParsed$results[[1]]$LATITUDE
    hdbresale_new[i,'LONGITUDE'] = jsonParsed$results[[1]]$LONGITUDE
  }
}
```

#### 4.1.3 Check if any rows with missing lat long

The following code chunk use is.na function of base package and sum to calculate the total number of NA is there is any.

```{r eval=FALSE}
sum(is.na(hdbresale_new$LATITUDE))
```

There are 26 rows with missing lat long. We will take a look at which areas are the affected rows

#### 4.1.4 Extract rows with missing lat long

The following code chunk filters the rows with missing latlong 

```{r eval=FALSE}
hdbresaleNA <- hdbresale_new %>%
                 filter(is.na(hdbresale_new$LATITUDE))
hdbresaleNA
```

From the extracted missing rows, the common location with missing lat long is ST. GEORGE. 

#### 4.1.5 Replace lat long with result for missing lat long

We will next assign the lat long to these rows as long as the address is "ST. GEORGE" if there are any result

Similar to 4.1.2, this time we set the address as default "ST.GEORGE" and assign to each row as long as there is result

```{r eval=FALSE}
for (i in 1:nrow(hdbresaleNA)) {
  address = "ST. GEORGE"
  url = paste('https://developers.onemap.sg/commonapi/search?searchVal=', address, '&returnGeom=Y&getAddrDetails=Y&pageNum=1')
  encoded_url <- URLencode(url)
  
  getcoordinate <- GET(encoded_url)
  
  jsonParsed <- content(getcoordinate,as="parsed")
  
  if (length(jsonParsed$results) > 0) {
    hdbresaleNA[i,'LATITUDE'] = jsonParsed$results[[1]]$LATITUDE
    hdbresaleNA[i,'LONGITUDE'] = jsonParsed$results[[1]]$LONGITUDE
  }
}
```

#### 4.1.6 Check again for missing lat long

The following code chunk check if there are still any missing value

```{r eval=FALSE}
hdbresaleNA
```

There are no missing value. However, the lat long for the 26 points are all the same.

#### 4.1.7 Jitter

In order to avoid having overlapping points, the following code chunk use jitter function of base package so that the lat long are distinct. factor is set to random small number so that the jitter would not go very far away from the area.

```{r eval=FALSE}
hdbresaleNA$LATITUDE <- jitter(as.numeric(hdbresaleNA$LATITUDE), factor = 0.00123)
hdbresaleNA$LONGITUDE <- jitter(as.numeric(hdbresaleNA$LONGITUDE), factor = 0.00123)
```

#### 4.1.7 Check again for missing lat long

```{r eval=FALSE}
hdbresaleNA
```

The lat long are now different.

#### 4.1.8 Extract rows without missing lat long

The following code chunk filters the rows with latlong using drop_na function of tidyr to drop rows with missing LATITUDE values

```{r eval=FALSE}
hdbresaleNOTNA <- hdbresale_new %>%
                      drop_na(LATITUDE)
hdbresaleNOTNA
```

#### 4.1.9 Combine rows with missing lat long and without missing lat long 

The following code chunk combines hdbresaleNA and hdbresaleNOTNA using rbind function of base package.

```{r eval=FALSE}
hdbresale_latlong <- rbind(hdbresaleNA, hdbresaleNOTNA)
glimpse(hdbresale_latlong)
```

We have 15901 rows which is same as the number of rows of the initial dataset. 

#### 4.1.9 Visualise the data on interactive map 

The following code chunk converts the dataset with latlong decimal degree to metres coordinates, and transform the crs to Singapore's projection, 3414.

```{r eval=FALSE}
hdbresale_latlongm <- st_as_sf(hdbresale_latlong,
                    coords = c("LONGITUDE", 
                               "LATITUDE"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

The following code chunk plots the points to see if the data is correct using functions of tmap package.

```{r eval=FALSE}
tmap_mode("view")
tm_shape(hdbresale_latlongm)+
  tm_dots()
tmap_mode("plot")
```


#### 4.1.10 Write as csv for backup

The following code chunk export data with sf as csv so that we do not have to re-run the time-consuming process above again.

```{r eval=FALSE}
write.csv(hdbresale_latlong, "data/newaspatial/hdbresale_latlongonly.csv", row.names = FALSE)
```

#### 4.1.11 hdb_update

The following code chunk import the latest hdb data so that the subsequent calculated data can added.

```{r eval=FALSE}
hdb_updated <- read_csv("data/aspatial/hdbresale_latlongonly.csv")
```

### 4.2 Proximity to CBD

Since CBD is a huge area, we will first define the lat long for CBD, taken from Google. (Latitude : 1.2801, Longitude : 103.8509)

#### 4.2.1 Get lat long

The following code chunk:

- iterate through every single rows in the data
- the start lat long, end lat long and the routeType are defined. In this analysis, we will be using routeType "drive"
- the url will be set and the space in url will be removed using gsub function of base package
- get the result of the url with GET function of httr package
- extract the content from the getproximitytocdb request. as argument is set as "parsed"
- if there are at least one result, we extract the total distance, convert it to km by dividing it by 1000, and assign it to the PROX_CBD column.

This process repeated a few times. This is because the onemapsgapi may return NA value for certain rows due to the request limit. Note that the first data was hdbresale_latlononly, and it was replaced by the data with missing distance, hdbresale_latlononly_CBD_NA_3, because of the repetition of this process.

```{r eval=FALSE}
for (i in 1:nrow(hdbresale_latlononly_CBD_NA3)) {
  startlat <- hdbresale_latlononly_CBD_NA3[i,'LATITUDE']
  startlong <- hdbresale_latlononly_CBD_NA3[i,'LONGITUDE']
  
  start = gsub(' ', '', paste(startlat, ',',  startlong))
  
  end = '1.2801,103.8509'
  routeType = 'drive'
  
  url = gsub(' ', '', paste('https://developers.onemap.sg/privateapi/routingsvc/route?start=', start, '&end=', end, '&routeType=', routeType, '&token=', token))
  
  getproximitytocbd <- GET(url)
  
  jsonParsed <- content(getproximitytocbd,as="parsed")
  
  if (length(jsonParsed$route_summary) > 0) {
    hdbresale_latlononly_CBD_NA3[i,'PROX_CBD'] = jsonParsed$route_summary$total_distance/1000
  }
}
```

#### 4.2.2 Write as csv to backup

The following code chunk write a copy of the csv for backup purposes. 

```{r eval=FALSE}
write.csv(hdbresale_latlononly_CBD_NA3, "data/newaspatial/hdbresale_PROXCBD_fifthrun.csv", row.names = FALSE)
```

#### 4.2.3 Extract rows with missing distance

The following code chunk filter the rows with distance NA, and repeat the above code chunk to obtain distance for remaining rows with NA value.

```{r eval=FALSE}
hdbresale_latlononly_CBD_NA4 <- hdbresale_latlononly_CBD_NA3 %>%
                              filter(is.na(hdbresale_latlononly_CBD_NA3$PROX_CBD))
hdbresale_latlononly_CBD_NA4
```

After a few iteration, hdbresale_latlononly_CBD_NA4 no longer have any data which means all rows have PROX_CBD distance.

#### 4.2.4 Extract rows without missing distance and write as csv to back up

The following code chunk filter the rows with valid distance value and write a copy of the csv in the file path

```{r eval=FALSE}
hdbresale_latlononly_CBD_NONA5 <- hdbresale_latlononly_CBD_NA3 %>%
                                  drop_na(PROX_CBD)
hdbresale_latlononly_CBD_NONA5

write.csv(hdbresale_latlononly_CBD_NONA5, "data/newaspatial/hdbresale_proxcbd5.csv", row.names = FALSE)
```

#### 4.2.5 Combine PROX_CBD data with valid distance

The following code chunk rbind all the dataset after getting the PROX_CBD of every row from the repetitive steps of 4.2.1 - 4.2.3. The final data with PROX_CBD is written as csv for backup

Data to bind includes:

- hdbresale_latlononly_CBD_NONA1, hdbresale_latlononly_CBD_NONA2, hdbresale_latlononly_CBD_NONA3, hdbresale_latlononly_CBD_NONA4 and hdbresale_latlononly_CBD_NONA5

```{r eval=FALSE}
PROX_CBD <- rbind(hdbresale_latlononly_CBD_NONA1, hdbresale_latlononly_CBD_NONA2, hdbresale_latlononly_CBD_NONA3, hdbresale_latlononly_CBD_NONA4, hdbresale_latlononly_CBD_NONA5)

PROX_CBD

write.csv(PROX_CBD, "data/newaspatial/hdbresale_proxcbdfinal.csv", row.names = FALSE)
```

#### 4.2.6 Combine data with hdb_updated 

The following code chunk combines PROX_CBD with the updated data, hdb_updated using cbind function of base package which combines column.

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_CBD)
```

### 4.3 Proximity to Eldercare

#### 4.3.1 Import Eldercare data

The following code chunk import eldercare shapefile data using st_read.

```{r eval=FALSE}
eldercare_sf <- st_read(dsn = "data/independentvar/extracted", layer="ELDERCARE")
```

There are 133 rows of eldercare. The projected CRS is already in SVY21.

#### 4.3.2 Calculate distances

The following code chunk:

- get the geometry from hdb and elderlycare sf
- retrieve coordinates in matrix form using st_coordinates function of sf package
- calculate the Euclidean distances (longlat set to FALSE) between hdb and elderlycare using spDists function of sp package

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

elderlycare_df <- eldercare_sf$geometry
elderlycare <- st_coordinates(elderlycare_df)

dist_hdb_elderlycare <- spDists(hdb, elderlycare, longlat=FALSE)
```

#### 4.3.3 Write as csv to backup

```{r eval=FALSE}
write.csv(dist_hdb_elderlycare, "data/newaspatial/hdbtoeldercare.csv", row.names = FALSE)
```

#### 4.3.4 Get min distances for each row

The following code chunk:

- converts dist_hdb_elderlycare into dataframe
- get the nearest Elderlycare distance for each hdb row using rowwise function of dplyr to check through a row and get the min across the row with X133 column using c_across function of dplyr, and create a Min column using mutate function of dplyr package. 
- create PROX_EDLDERCARE and filter all the min value converted into km.

```{r eval=FALSE}
dist_hdb_elderlycare_df <- data.frame(dist_hdb_elderlycare)

dist_hdb_elderlycare_min <- dist_hdb_elderlycare_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X133)))

PROX_ELDERLYCARE <- dist_hdb_elderlycare_min$Min/1000
```

#### 4.3.5 Combine data with hdb_updated

The following code chunk combines PROX_ELDERLYCARE with the updated data, hdb_updated, via cbind function of base package.

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_ELDERLYCARE)
```

#### 4.3.6 Write as csv to backup

The following code chunk writes the updated data for backup purposes.

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated1.csv", row.names = FALSE)
```

### 4.4 Proximity to Hawker Centre

#### 4.4.1 Import Hawker data

The following code chunk import and assign the projection crs of Singapore 3414.

```{r eval=FALSE}
hawkercentre_sf <- st_read("data/independentvar/extracted/hawker-centres-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 125 rows of Hawker Centre.

#### 4.4.2 Calculate distances

The following code chunk is similar to 4.3.2. The only difference is that we only use the hawkercentre's first and second column (X and Y coordinate) since the st_coordinates also include the Z coordinate column in the data.

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

hawkercentre_df <- hawkercentre_sf$geometry
hawkercentre <- st_coordinates(hawkercentre_df)
hawkercentre <- hawkercentre[,c(1,2)]

dist_hdb_hawkercentre <- spDists(hdb, hawkercentre, longlat=FALSE)
```

#### 4.4.3 Write as csv to backup

The following code chunk is similar to 4.3.3.

```{r eval=FALSE}
write.csv(dist_hdb_hawkercentre, "data/newaspatial/hdbtohawker.csv", row.names = FALSE)
```

#### 4.4.4 Get min distances for each row

The following code chunk is similar to 4.3.4.

```{r eval=FALSE}

dist_hdb_hawkercentre_df <- data.frame(dist_hdb_hawkercentre)

dist_hdb_hawkercentre_min <- dist_hdb_hawkercentre_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X125)))

PROX_HAWKER <- dist_hdb_hawkercentre_min$Min/1000
```

#### 4.4.5 Combine data with hdb_updated

The following code chunk is similar to 4.3.5

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_HAWKER)
```

#### 4.4.6 Write as csv to backup

The following code chunk is similar to 4.3.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated2.csv", row.names = FALSE)
```

### 4.5 Proximity to MRT

#### 4.5.1 Import MRTLRT data

The following code chunk import the MRT shapefile using st_read.

```{r eval=FALSE}
mrtlrt_sf <- st_read(dsn = "data/independentvar/extracted", layer="MRTLRTStnPtt")
```

There are 185 rows of MRTLRT data. The project CRS is already SVY21.

#### 4.5.2 Calculate distances

The following code chunk is similar to 4.3.2.

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

mrtlrt_df <- mrtlrt_sf$geometry
mrtlrt <- st_coordinates(mrtlrt_df)

dist_hdb_mrtlrt <- spDists(hdb, mrtlrt, longlat=FALSE)
```

#### 4.5.3 Write as csv to backup

The following code chunk is similar to 4.3.3

```{r eval=FALSE}
write.csv(dist_hdb_mrtlrt, "data/newaspatial/hdbtomrtlrt.csv", row.names = FALSE)
```

#### 4.5.4 Get min distances for each row

The following code chunk is similar to 4.3.4

```{r eval=FALSE}
dist_hdb_mrtlrt_df <- data.frame(dist_hdb_mrtlrt)

dist_hdb_mrtlrt_min <- dist_hdb_mrtlrt_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X185)))

PROX_MRTLRT <- dist_hdb_mrtlrt_min$Min/1000
```

#### 4.5.5 Combine data with hdb_updated

The following code chunk is similar to 4.3.5

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_MRTLRT)
```

#### 4.5.6 Write as csv to backup

The following code chunk is similar to 4.3.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated3.csv", row.names = FALSE)
```

### 4.6 Proximity to Parks

#### 4.6.1 Import Parks data

The following code chunk import and assign the projection crs of Singapore 3414.

```{r eval=FALSE}
parks_sf <- st_read("data/independentvar/extracted/parks-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 350 rows of parks. 

#### 4.6.2 Calculate distances

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

parks_df <- parks_sf$geometry
parks <- st_coordinates(parks_df)
parks <- parks[,c(1,2)]

dist_hdb_parks <- spDists(hdb, parks, longlat=FALSE)
```

#### 4.6.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_parks, "data/newaspatial/hdbtoparks.csv", row.names = FALSE)
```

#### 4.6.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_parks_df <- data.frame(dist_hdb_parks)

dist_hdb_parks_min <- dist_hdb_parks_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X350)))

PROX_PARKS <- dist_hdb_parks_min$Min/1000
```

#### 4.6.5 Combine data with hdb_updated

The following code chunk is similar to 4.4.5

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_PARKS)
```

#### 4.6.6 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated4.csv", row.names = FALSE)
```

### 4.7 Proximity to Supermarket

#### 4.7.1 Import Supermarket data

The following code chunk import data and assign the projection crs of Singapore 3414.

```{r eval=FALSE}
supermarkets_sf <- st_read("data/independentvar/extracted/supermarkets-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 526 rows of supermarket.

#### 4.7.2 Calculate distances

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

supermarkets_df <- supermarkets_sf$geometry
supermarkets <- st_coordinates(supermarkets_df)
supermarkets <- supermarkets[,c(1,2)]

dist_hdb_supermarkets <- spDists(hdb, supermarkets, longlat=FALSE)
```

#### 4.7.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_supermarkets, "data/newaspatial/hdbtosupermarkets.csv", row.names = FALSE)
```

#### 4.7.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_supermarkets_df <- data.frame(dist_hdb_supermarkets)

dist_hdb_supermarkets_min <- dist_hdb_supermarkets_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X526)))

PROX_SUPERMARKETS <- dist_hdb_supermarkets_min$Min/1000
```

#### 4.7.5 Combine data with hdb_updated

The following code chunk is similar to 4.4.5

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_SUPERMARKETS)
```

#### 4.7.6 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated5.csv", row.names = FALSE)
```

### 4.8 Proximity to CHAS clinics

#### 4.8.1 Import CHAS clinics data

The following code chunk imports data and assigned projection crs of Singapore 3414.

```{r eval=FALSE}
clinics_sf <- st_read("data/independentvar/extracted/chas-clinics-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 1167 rows of CHAS clinics.

#### 4.8.2 Calculate distances

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

clinics_df <- clinics_sf$geometry
clinics <- st_coordinates(clinics_df)
clinics <- clinics[,c(1,2)]

dist_hdb_clinics <- spDists(hdb, clinics, longlat=FALSE)
```

#### 4.8.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_clinics, "data/newaspatial/hdbtoclinics.csv", row.names = FALSE)
```

#### 4.8.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_clinics_df <- data.frame(dist_hdb_clinics)

dist_hdb_clinics_min <- dist_hdb_clinics_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X1167)))

PROX_CLINICS <- dist_hdb_clinics_min$Min/1000
```

#### 4.8.5 Combine data with hdb_updated

The following code chunk is similar to 4.4.5

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_CLINICS)
```

#### 4.8.6 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated6.csv", row.names = FALSE)
```

### 4.9 Proximity to Kindergartens and Number of Kindergartens within 350m

#### 4.9.1 Import kindergartens data

The following code chunks import data and assigned projected crs of 3414.

```{r eval=FALSE}
kindergartens_sf <- st_read("data/independentvar/extracted/kindergartens-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 448 rows of kindergartens.

#### 4.9.2 Calculate distances

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

kindergartens_df <- kindergartens_sf$geometry
kindergartens <- st_coordinates(kindergartens_df)
kindergartens <- kindergartens[,c(1,2)]

dist_hdb_kindergartens <- spDists(hdb, kindergartens, longlat=FALSE)
```

#### 4.9.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_kindergartens, "data/newaspatial/hdbtokindergartens.csv", row.names = FALSE)
```

#### 4.9.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_kindergartens_df <- data.frame(dist_hdb_kindergartens)

dist_hdb_kindergartens_min <- dist_hdb_kindergartens_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X448)))

PROX_KINDERGARTENS <- dist_hdb_kindergartens_min$Min/1000
```

#### 4.9.5 Count the number of point below 350m in each row

The following code chunk calculates the number of points with distance lower than 350m, which means number of points within 350m, using rowSum function of base package.

```{r eval=FALSE}
dist_hdb_kindergartens_df$num_within_350m <- rowSums(dist_hdb_kindergartens_df <= 350)

NUM_KINDERGARDEN_350M <- dist_hdb_kindergartens_df$num_within_350m
```

#### 4.9.6 Combine data with hdb_updated

The following code chunk combines PROX_KINDERGARTENS and NUM_KINDERGARDEN_350M with the most updated data, hdb_updated, via cbind function of base package.

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_KINDERGARTENS, NUM_KINDERGARDEN_350M)
```

#### 4.9.7 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated7.csv", row.names = FALSE)
```

### 4.10  Proximity to Childcare centres and Numbers of Childcare centre within 350m

#### 4.10.1 Import childcare centres data

The following code chunks import data and assigned projected crs of 3414.

```{r eval=FALSE}
childcare_sf <- st_read("data/independentvar/extracted/child-care-services-geojson.geojson") %>%
                    st_transform(crs = 3414)
```

There are 1545 rows of childcare centres.

#### 4.10.2 Calculate distances

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

childcare_df <- childcare_sf$geometry
childcare <- st_coordinates(childcare_df)
childcare <- childcare[,c(1,2)]

dist_hdb_childcare <- spDists(hdb, childcare, longlat=FALSE)
```

#### 4.10.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_childcare, "data/newaspatial/hdbtochildcare.csv", row.names = FALSE)
```

#### 4.10.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_childcare_df <- data.frame(dist_hdb_childcare)

dist_hdb_childcare_min <- dist_hdb_childcare_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X1545)))

PROX_CHILDCARE <- dist_hdb_childcare_min$Min/1000
```

#### 4.10.5 Count the number of point below 350m in each row

The following code chunk is similar to 4.9.5

```{r eval=FALSE}
dist_hdb_childcare_df$num_within_350m <- rowSums(dist_hdb_childcare_df <= 350)

NUM_CHILDCARE_350M <- dist_hdb_childcare_df$num_within_350m
```

#### 4.10.6 Combine data with hdb_updated

The following code chunk is similar to 4.9.6

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_CHILDCARE, NUM_CHILDCARE_350M)
```

#### 4.10.7 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated8.csv", row.names = FALSE)
```

### 4.11 Proximity to Bus stop and Numbers of Bus stop within 350m

#### 4.11.1 Import bus stops data

The following code chunk imports busstop shapefile. 

```{r eval=FALSE}
busstop_sf <- st_read(dsn = "data/independentvar/extracted", layer="BusStop")
```

There are 5156 rows of eldercare. The projected CRS is already in SVY21.

#### 4.11.2 Calculate distance

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

busstop_df <- busstop_sf$geometry
busstop <- st_coordinates(busstop_df)
busstop <- busstop[,c(1,2)]

dist_hdb_busstop <- spDists(hdb, busstop, longlat=FALSE)
```

#### 4.11.3 Write as csv to backup

The following code chunk is similar to 4.4.3

```{r eval=FALSE}
write.csv(dist_hdb_busstop, "data/newaspatial/hdbtobusstop.csv", row.names = FALSE)
```

#### 4.11.4 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_busstop_df <- data.frame(dist_hdb_busstop)

dist_hdb_busstop_min <- dist_hdb_busstop_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X5156)))

PROX_BUSSTOP <- dist_hdb_busstop_min$Min/1000
```

#### 4.11.5 Count the number of point below 350m in each row

The following code chunk is similar to 4.9.5

```{r eval=FALSE}
dist_hdb_busstop_df$num_within_350m <- rowSums(dist_hdb_busstop_df <= 350)
NUM_BUSSTOP_350M <- dist_hdb_busstop_df$num_within_350m
```

#### 4.11.6 Combine data with hdb_updated

The following code chunk is similar to 4.9.6

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_BUSSTOP, NUM_BUSSTOP_350M)
```

#### 4.11.7 Write as csv to backup

The following code chunk is similar to 4.4.6

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated9.csv", row.names = FALSE)
```

### 4.12 Proximity to Primary school and Numbers of Primary school within 1km

#### 4.12.1 Import school data

The following code chunk reads the csv file of school using read_csv function of readr package.

```{r eval=FALSE}
schools_df <- read_csv("data/independentvar/extracted/Singapore+Schools-2021-10-21.csv")
```

There are 367 rows of schools data.

#### 4.12.2 Filter Primary school

The following code chunk filters rows that contains PRIMRY SCHOOL in the school name using grepl function of base 

```{r eval=FALSE}
primary_schools_df <- schools_df %>% filter(grepl('PRIMARY SCHOOL', Name))
```

#### 4.12.3 Get Lat Long for Primary School with postalcode

The following code chunk is similar to 4.1.2

```{r eval=FALSE}
for (i in 1:nrow(primary_schools_df)) {
  postalcode <- primary_schools_df[i,'Zip/Postal Code']
  
  url = paste('https://developers.onemap.sg/commonapi/search?searchVal=', postalcode, '&returnGeom=Y&getAddrDetails=Y&pageNum=1')
  encoded_url <- URLencode(url)
  
  getcoordinate <- GET(encoded_url)
  
  jsonParsed <- content(getcoordinate,as="parsed")
  
  if (length(jsonParsed$results) > 0) {
    primary_schools_df[i,'LATITUDE'] = jsonParsed$results[[1]]$LATITUDE
    primary_schools_df[i,'LONGITUDE'] = jsonParsed$results[[1]]$LONGITUDE
  }
}
```

#### 4.12.4 Check NA

The following code chunk checks if there are any rows with missing Latitude.

```{r eval=FALSE}
sum(is.na(primary_schools_df$LATITUDE))
```

There are three rows of schools with missing NA.

#### 4.12.5 Assign latlong manually

The following code chunk assign lat long manually (searched from google) to the rows with missing LATITUDE and LONGITUDE

```{r eval=FALSE}
primary_schools_df[8,'LATITUDE'] = '1.2753515360141332'
primary_schools_df[8,'LONGITUDE'] = '103.83994593723308'

primary_schools_df[37,'LATITUDE'] = '1.2751443823249178'
primary_schools_df[37,'LONGITUDE'] = '103.82391208407438'

primary_schools_df[100,'LATITUDE'] = '1.3252154625658445'
primary_schools_df[100,'LONGITUDE'] = '103.88172874174562'
```

#### 4.12.6 Select only necessary column

The following code chunk only selects necessary column.

```{r eval=FALSE}
primary_schools_df_selected <- primary_schools_df %>% select('Name', 'Address', 'Zip/Postal Code', 'LATITUDE', 'LONGITUDE')
```

There are 156 rows of primary school after filtering.

#### 4.12.7 Write as csv to backup

```{r eval=FALSE}
write.csv(primary_schools_df_selected, "data/newaspatial/primaryschoollatlong.csv", row.names = FALSE)
```

#### 4.12.8 Convert Latlong to geometry

The following code chunk converts the dataset with latlong decimal degree to metres coordinates, and transform the crs to Singapore's projection, 3414.

```{r eval=FALSE}
primary_schools_sf <- st_as_sf(primary_schools_df_selected,
                    coords = c("LONGITUDE", 
                               "LATITUDE"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### 4.12.9 Calculate distance

The following code chunk is similar to 4.4.2

```{r eval=FALSE}
hdb_updated_geometry <- st_as_sf(hdb_updated,
                          coords = c("LONGITUDE", 
                                     "LATITUDE"),
                          crs=4326) %>%
                          st_transform(crs = 3414)

hdb_df <- hdb_updated_geometry$geometry
hdb <- st_coordinates(hdb_df)

prisch_df <- primary_schools_sf$geometry
prisch <- st_coordinates(prisch_df)
prisch <- prisch[,c(1,2)]

dist_hdb_prisch <- spDists(hdb, prisch, longlat=FALSE)
```

#### 4.12.10 Write as csv to backup

```{r eval=FALSE}
write.csv(dist_hdb_prisch, "data/newaspatial/hdbtoprisch.csv", row.names = FALSE)
```

#### 4.12.11 Get min distances for each row

The following code chunk is similar to 4.4.4

```{r eval=FALSE}
dist_hdb_prisch_df <- data.frame(dist_hdb_prisch)

dist_hdb_prisch_min <- dist_hdb_prisch_df %>% rowwise() %>% mutate(Min = min(c_across(X1:X156)))

PROX_PRISCH <- dist_hdb_prisch_min$Min/1000
```

#### 4.12.12 Count the number of point below 1km in each row

The following code chunk is similar to 4.9.5. The only difference is that we are counting the number of point below 1km.

```{r eval=FALSE}
dist_hdb_prisch_df$num_within_1km <- rowSums(dist_hdb_prisch_df <= 1000)

NUM_PRISCH_1KM <- dist_hdb_prisch_df$num_within_1km
```

#### 4.12.13 Combine data with hdb_updated

The following code chunk is similar to 4.9.6

```{r eval=FALSE}
hdb_updated <- cbind(hdb_updated, PROX_PRISCH, NUM_PRISCH_1KM)
```

#### 4.12.14 Write as csv to backup

```{r eval=FALSE}
write.csv(hdb_updated, "data/newaspatial/hdbupdated10.csv", row.names = FALSE)
```

### 4.13 Storey Level Dummy Variable

The following code chunk creates dummy variable for storey_range column using dummy function of dummies package.

```{r eval=FALSE}
hdb_updated1 <- cbind(hdb_updated, dummy(hdb_updated$storey_range, sep = "_")) 
```

The following code chunk replace the dummy's column name from "hdb_updated_01 TO 03" format to "storey_01TO03" format using gsub function of base package.

```{r eval=FALSE}
names(hdb_updated1) = gsub("hdb_updated_", "storey_", names(hdb_updated1))
names(hdb_updated1) = gsub(" ", "", names(hdb_updated1))
```

The following code chunk then select all the require column in hdb_updated1 using select function and assign it to hdb_final data frame.

```{r eval=FALSE}
hdb_final <- hdb_updated1 %>%
            select(address, storey_range, LATITUDE, LONGITUDE, resale_price, floor_area_sqm, remaining_lease_year, PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRTLRT, PROX_PARKS, PROX_SUPERMARKETS, PROX_CLINICS, NUM_KINDERGARDEN_350M, NUM_CHILDCARE_350M, NUM_BUSSTOP_350M, NUM_PRISCH_1KM, `storey_01TO03`, `storey_04TO06`, `storey_07TO09`, `storey_10TO12`, `storey_13TO15`, `storey_16TO18`, `storey_19TO21`, `storey_22TO24`, `storey_25TO27`, `storey_28TO30`, `storey_31TO33`, `storey_34TO36`, `storey_37TO39`, `storey_40TO42`, `storey_43TO45`, `storey_46TO48`, `storey_49TO51`)
```

The following code chunk writes hdb_final simple feature dataframe as csv using write.csv function

```{r eval=FALSE}
write.csv(hdb_final, "data/aspatial/hdb_final.csv", row.names = FALSE)
```

## 5. Data Import

### 5.1 Geospatial Data

The following code chunk import MP14_SUBZONE_WEB_PL using st_read function of sf package.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

The following code chunk update the EPSG code 3414 using st_transform of sf package and retrieve coordinate using st_crs of sf package

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
st_crs(mpsz_svy21)
```

We can see that the EPSG is 3414 which is correct.

### 5.2 Aspatial data

#### 5.2.1 hdb_final data

The following code chunk read hdb_final csv file as tibble data frame using read_csv function and glimpse to display the data 

```{r}
hdb_final <- read_csv("data/aspatial/hdb_final.csv")
glimpse(hdb_final)
```

There are 15901 rows of data and 35 columns as shown above. 

#### 5.2.2 Check if any NA

The following code chunk Checks if there are any missing values in the data using sum and is.na function of base package

```{r}
sum(is.na(hdb_final))
```

There are no missing values in the data.

### 5.3 Convert aspatial data frame to simple feature data frame

The following code chunk converts hdb_final dataframe into sf dataframe using st_as_sf function of sf package. The coordinate is then converted into svy21 EPSG:3414 FROM WGS84 using st_transform function.

```{r}
hdb_final_sf <- st_as_sf(hdb_final,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
                  st_transform(crs=3414)
head(hdb_final_sf)
```

## 6. EDA

### 6.1 Statistical Graph

#### 6.1.1 Resale Price

The following code chunk plots the distribution of resale_price using ggplot function of ggplot2 package.

```{r}
ggplot(data=hdb_final_sf, aes(x= `resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

From the above, we can see a normal distribution. Majority of the four room hdb were transacted for about $300000 to 400000.

#### 6.1.2 Independent variables

Next, we are interested to see the distribution of independent variables.

The following code chunk plots multiple distribution of other variables using ggplot function of ggplot2 package and arrange them in columns of 3 using ggarrange function of ggpubr package. Note that NUM_KINDERGARTENS_350M, NUM_CHILDCARE_350M, NUM_BUSSTOPS_350M and NUM_PRISCH_1KM are not plotted because the data is in count.

```{r}
floor_area_sqm <- ggplot(data=hdb_final_sf, aes(x= `floor_area_sqm`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

remaining_lease_year <- ggplot(data=hdb_final_sf, aes(x= `remaining_lease_year`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=hdb_final_sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=hdb_final_sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER <- ggplot(data=hdb_final_sf, aes(x= `PROX_HAWKER`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRTLRT <- ggplot(data=hdb_final_sf, aes(x= `PROX_MRTLRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARKS <- ggplot(data=hdb_final_sf, aes(x= `PROX_PARKS`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_SUPERMARKETS <- ggplot(data=hdb_final_sf, aes(x= `PROX_SUPERMARKETS`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CLINICS <- ggplot(data=hdb_final_sf, aes(x= `PROX_CLINICS`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(floor_area_sqm, remaining_lease_year, PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRTLRT, PROX_PARKS, PROX_SUPERMARKETS, PROX_CLINICS, ncol = 3, nrow = 3)
```

From the above, we can see that most of variables are normally distributed. The remaining lease year seems to be normally distributed but the only exception is that it has a high count of transacted four-room hdb with remaining_lease_year of above 90 years. There is no need for any transformation since the independent variables are normally distributed.

### 6.2 Statiscal Point Map

Resale Price

The following code chunk:

- plots the resale price point on an interactive mode
- mpsz_svy21 as base layer and hdb_final_sf points on top of mpsz_svy21 layer using function of tmap package
- the style of dots is set to quantile
- tm_view sets the min zoom level as 10 and max zoom level as 15

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons()+
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_sf) +  
  tm_dots(col = "resale_price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(10,15))
tmap_mode("plot")
```

From the interactive map, we can see that hdb in central and south area tends to have higher resale price ranging from 500000 to 1186888 (darker red points) while hdb in the west and north have lower resale price ranging from 218000 to 340000 as seen that there are more light yellow points in the west and north.

## 7. Hedonic Pricing Model

### 7.1 Multiple Linear Regression Model

#### 7.1.1 Relationship between Independent variables

To avoid multicolinearity, we need to ensure that the independent variables are not highly correlated before we build a multiple regression model.

The following code chunk:

- compute correlation using cor function of stats package for 6th column variable (floor_area_sqm) to the last column variable (storey49TO51) of hdb_final.
- visualise the correlation matrix using corrplot function of corrplot package
- The order argument is set to AOE which orders the variables using angular order of the eigenvectors methods
- t1.cex is set to 0.7 so that the variables are more visible, number.cex is set to 0.5 so that the number within each box will not exceed the box.

```{r fig.width=8, fig.height=8}
corrplot(cor(hdb_final[, 6:35]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.7, number.cex=0.5, method = "number", type = "upper")
```

From the scatterplot above, we can see that the independent variables are not highly correlated to each other. Hence, we can include all of them in the subsequent model building.

#### 7.1.2 Hedonic pricing model using multiple linear regression method

The following code chunk builds a multiple linear regression model using lm function of stats package.

Note that storey49TO51 dummy variable is excluded since because we should only have n-1 dummy variable. It will have a strong correlation with other independent variable if we were to include it.

```{r}
hdb_final_mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + PROX_CLINICS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M + NUM_PRISCH_1KM + storey_01TO03 + storey_04TO06 + storey_07TO09 + storey_10TO12 + storey_13TO15 + storey_16TO18 + storey_19TO21 + storey_22TO24 + storey_25TO27 + storey_28TO30 + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42 + storey_43TO45 + storey_46TO48, data=hdb_final_sf)

summary(hdb_final_mlr)
```

From the above, we can see that there are some independent variables that are statistically not significant. Variables such as PROX_CLINICS, storey_43TO45 and storey_46TO48 have p-value above 0.05 and are not statistically significant. We will exclude these variable when we calibrate the revised model.

The following code chunk:

- calibrate the revised model using lm function with variables that are not statistically significant removed. 
- Ordinary least squares regression is then done by running ols_regress function of olsrr package.

```{r}
hdb_final_mlr1 <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M + NUM_PRISCH_1KM + storey_01TO03 + storey_04TO06 + storey_07TO09 + storey_10TO12 + storey_13TO15 + storey_16TO18 + storey_19TO21 + storey_22TO24 + storey_25TO27 + storey_28TO30 + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sf)

ols_regress(hdb_final_mlr1)
```

From the above, the adjusted R-squared for the revised multiple linear regression model is 0.719.

#### 7.1.3 Checks for multicolinearity

The following code chunk test for sign of multicolinearity using ols_vif_tol function of olsrr package.

```{r}
ols_vif_tol(hdb_final_mlr1)
```

There are some independent variables with VIF value above 10 which means that there are sign of multicollinearity. Hence we would want to calibrate the model again by removing independent variables with VIF above 10. Independent variables to be removed includes storey_01TO03, storey_04TO06, storey_07TO09, storey_10TO12, storey_13TO15, storey_16TO18, storey_19TO21, storey_22TO24, storey_25TO27 and storey_28TO30.

Revise the model

The following code chunk:

- calibrate the revised model using lm function with variables that have a VIF value above 10 removed. 
- Ordinary least squares regression is then done by running ols_regress function of olsrr package.

```{r}
hdb_final_mlr2 <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M + NUM_PRISCH_1KM + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sf)
```

Checks for multicolinearity again

```{r}
ols_vif_tol(hdb_final_mlr2)
```

From the above, We can conclude that there are no sign of multicollinearity among the independent variables since the VIF value are below 10.

Ordinary least squares regression 

The following code chunk run ols_regress function of olsrr package for the new model hdb_final_mlr2

```{r}
ols_regress(hdb_final_mlr2)
```

From the above, the adjusted R-squared for the revised multiple linear regression model is 0.681, lower than the initial revised model's adjusted R-square value of 0.719.

#### 7.1.4 Test for Non-Linearity

The following code chunk checks the linear relationsihp assumption using ols_plot_resid_fit function of olsrr.

```{r}
ols_plot_resid_fit(hdb_final_mlr2)
```

From the above, most of the data point are scattered near the 0 line. Around the middle area with fitted value of 0.00006, there are slightly more data points with residual above 0. In general, we can conclude that the relationships between the dependent variable and independent variables are linear.

#### 7.1.5 Test for Normality Assumption

The following code chunk checks normality assumption using ols_plot_resid_hist function of oslrr package

```{r}
ols_plot_resid_hist(hdb_final_mlr2)
```

From the above, we can see that the residual of the revised multiple linear regression model resembles a normal distribution.

#### 7.1.6 Testing for Spatial Autocorrelation

To perform spatial autocorrelation test, we need to convert hdb_final_sf simple into a SpatialPointsDataFrame.

Export residuals

The following code chunk export the residuals of the multiple linear regression model we have build and save it as data frame

```{r}
mlr.output <- as.data.frame(hdb_final_mlr2$residuals)
```

Combine data frame

The following code chunk combines the residual output data frame with hdb_final_sf data frame using cbind function of base package, and rename the residual column to MLR_RES.

```{r}
hdb_final_res_sf <- cbind(hdb_final_sf, mlr.output) %>%
                        rename(`MLR_RES` = `hdb_final_mlr2.residuals`)
```

Convert hdb_final_res_sf into SpatialPointsDataFrame

The following code chunk converts hdb_final_res_sf into SpatialPointsDataFrame using as_Spatial function of sf package.

```{r}
hdb_final_sp <- as_Spatial(hdb_final_res_sf)
hdb_final_sp
```

Visualise the distribution of residuals

The following code chunk plots the distribution of residuals on an interactive map with mpsz_svy21 layer.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons()+
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_res_sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(10,15))
tmap_mode("plot")
```

From the above interactive plot, we can see that the residuals are not randomly distributed as the residual points seems to be clustered, with many of the same colour points near each other. This means that there is a sign of spatial autocorrelation.

Perform Moran’s I test

We can perform Moran’s I test to check if there is sign of spatial autocorrelation.

Compute distance-based weight matrix

The following code chunk first compute the distance-based weight matrix by using dnearneigh function of spdep. The lower distance bound is set to 0m, upper distance bound set to 1500m, and longlat set to FALSE which means euclidean distance will be calculated.

```{r}
nb <- dnearneigh(coordinates(hdb_final_sp), 0, 1500, longlat = FALSE)
summary(nb)
```

From the above, we can see that the three least connected regions are row 2914, 3384 and 9808 with 2 links each. The most connected regions are row 1901, 7609, 10897 and 12072 with 1971 links each.

Convert output neighbour into spatial weight

The following code chunk converts nb into spatial weight using nb2listw function of spdep package.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

Perform Moran’s I test

The following code chunk performs Moran’s I test using lm.morantest function of spdep package. It compares the multiple linear regression model and the spatial weight.

```{r}
lm.morantest(hdb_final_mlr2, nb_lw)
```

From the above, the p-value is lower than 2.2e-16, which is lesser than alpha value of 0.05. Therefore, we reject the null hypothesis that residual are randomly distributed. In addition, the Observed Global Moran I test is 0.38 and it is greater than 0. We can conclude that the residual resembles a clustering distribution.

## 8. Building Hedonic Pricing Models using GWmodel

In this section, we will build the Hedonic Pricing Models using Fixed and Adaptive GWR model, with the same variables in the revised multiple linear regression model, hdb_final_mlr2. 

### 8.1 Fixed Bandwidth GWR model

#### 8.1.1 Computing fixed bandwidth

The following code chunk determines the optimal fixed bandwidth for the model using bw.gwr function of GWmodel package.

The following are the arguments: 
- Approach set as Cross-validation ("CV")
- Weighting function set as Guassian
- Adaptive set as False since we are computing fixed bandwidth
- longlat set as False to calculate Euclidean distance

```{r eval=FALSE}
bw.fixed <- bw.gwr(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M +
NUM_PRISCH_1KM + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sp, approach="CV", kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

The output of the above code chunk will shown in screenshot format due to the long run time.

![bw.fixed Screenshot](data/screenshots/bwfixed.jpg)

From the above screenshot, we can see that the recommended fixed bandwidth is 350.8655 metres.

#### 8.1.2 GWModel method

The following code chunk calibrate the gwr model using gwr.basic function of GWmodel with fixed bandwidth and gaussian kernel.

```{r eval=FALSE}
gwr.fixed <- gwr.basic(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M +
NUM_PRISCH_1KM + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sp, bw=bw.fixed, kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

```{r eval=FALSE}
gwr.fixed
```

The output of the above code chunk will shown in screenshot format using the code chunk below due to the long run time.

![bw.fixed GWR Screenshot1](data/screenshots/gwrfixed1.jpg)

![bw.fixed GWR Screenshot2](data/screenshots/gwrfixed2.jpg)

![bw.fixed GWR Screenshot3](data/screenshots/gwrfixed3.jpg)

From the fixed bandwidth GWR model, the adjusted R-square value is 0.941, higher than the revised multiple linear regression model with adjusted R-square value of 0.681.

### 8.2 Adaptive Bandwidth GWR model

#### 8.2.1 Computing adaptive bandwidth

The following code chunk determines the optimal adaptive bandwidth for the model using bw.gwr function of GWmodel package. It is similar to 8.1.1, the only difference is that the adaptive argument is set to TRUE since we are computing adaptive bandwidth.

```{r eval=FALSE}
bw.adaptive <- bw.gwr(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M +
NUM_PRISCH_1KM + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sp, approach="CV", kernel="gaussian", adaptive=TRUE, longlat=FALSE)
```

The output of the above code chunk will shown in screenshot format due to the long run time.

![bw.adaptive Screenshot](data/screenshots/bwadaptive.jpg)

From the screenshot above, we can see that the recommended adaptive bandwidth is 209 metres.

#### 8.2.2 GWModel method

The following code chunk calibrate the gwr model using gwr.basic function of GWmodel with adaptive bandwidth and gaussian kernel.

```{r eval=FALSE}
gwr.adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + remaining_lease_year + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
PROX_MRTLRT + PROX_PARKS + PROX_SUPERMARKETS + NUM_KINDERGARDEN_350M + NUM_CHILDCARE_350M + NUM_BUSSTOP_350M +
NUM_PRISCH_1KM + storey_31TO33 + storey_34TO36 + storey_37TO39 + storey_40TO42, data=hdb_final_sp, bw=bw.adaptive, kernel="gaussian", adaptive=TRUE, longlat=FALSE)
```

```{r eval=FALSE}
gwr.adaptive
```

The output of the above code chunk will shown in screenshot format using the code chunk below because the run time is very long.

![bw.adaptive GWR Screenshot1](data/screenshots/gwradaptive1.jpg)

![bw.adaptive GWR Screenshot2](data/screenshots/gwradaptive2.jpg)

From the adaptive bandwidth GWR model, the adjusted R-square value is 0.914, higher than the revised multiple linear regression model with adjusted R-square value of 0.681, but lower than the fixed bandwidth GWR Model with adjusted R-square value of 0.941.

Comparing Revised multiple linear regression model, fixed bandwidth GWR model and adaptive bandwidth GWR model, the fixed bandwidth GWR model has the best adjusted R-square of 0.941. Hence, we will use it to visualise GWR output in the next section.

## 9. Visualising GWR Output

### 9.1 Convert SDF into sf data frame

The following code chunk:

- converts SDF of gwr.fixed into sf object using st_as_sf function of sf package
- transform the coordinate to 3414
- assign it to new sf object name hdb_final_sf_fixed

```{r eval=FALSE}
hdb_final_sf_fixed <- st_as_sf(gwr.fixed$SDF) %>%
                        st_transform(crs=3414)
```

The following code chunk display the columns in hdb_final_sf_fixed sf object using glimpse function of pillar package

```{r eval=FALSE}
glimpse(hdb_final_sf_fixed)
```

We will write the hdb_final_sf_fixed files into csv using st_write function of sf package and convert the geometry into X and Y column using the layer_options argument

```{r eval=FALSE}
st_write(hdb_final_sf_fixed, "data/aspatial/hdb_final_sf_fixed.csv", layer_options = "GEOMETRY=AS_XY")
```

Import hdb_final_sf_fixed file for visualization

The following code chunk imports hdb_final_sf_fixed file using read_csv function.

```{r}
hdb_final_sf_fixed <- read_csv("data/aspatial/hdb_final_sf_fixed.csv")
```

Convert to simple feature data frame

The following code chunk:

- converts hdb_final_sf_fixed data frame into sf data frame using st_as_sf
- X and Y column as coords argument
- crs set to 3414

```{r}
hdb_final_sf_fixed1 <- st_as_sf(hdb_final_sf_fixed,
                          coords = c("X", 
                                     "Y"),
                          crs=3414)
```

The following code chunk display the columns of hdb_final_sf_fixed1 simple feature object using glimpse function of pillar package

```{r}
glimpse(hdb_final_sf_fixed1)
```

From the above, there are 15091 rows. We can also see the gwr outputs such as Local_R2, residuals, coefficient standard error, observed and predicted y values. We will visualise some of them in the next section. 

### 9.2 Visualization and Summary Statistics

#### 9.2.1 Local_R2

Visualization

The following code chunk plots the interactive point maps of Local_R2 using tmap package function.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons() +
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_sf_fixed1) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(10,15))
tmap_mode("plot")
```

From the Local_R2 map, we can see that in general, the fixed GWR model predicts well as shown by many red points on the map. Region such as North-east and East are predicted poorly by the fixed GWR model as there are a few yellow points and some orange points in these two region, and this means that the Local_R2 value is low and ranges from 0.2 to 0.4, and 0.6 to 0.8 respectively. We would want to analyse these region further.

Summary statistics

The following code chunk checks the summary statistics of Local_R2 using summary function

```{r}
summary(hdb_final_sf_fixed1$Local_R2)
```

From the summary statistics, the min is 0.2329 and the max is 1. The 1st quartile and median is already 0.8321 and 0.915 respectively which closer to 1. The mean is also close to 1 at 0.8661. This shows that the Local_R2 are generally high, but have some low values as seen from the large difference between the min and first quartile, 0.2329 to 0.8321 respectively.


##### 9.2.1.1 Local_R2 by planning area

North-east

The following code chunk plots an interactive map within the North-east planning area.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="NORTH-EAST REGION", ])+
  tm_polygons() +
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_sf_fixed1) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(10,15))
tmap_mode("plot")
```

In the North-east region, we can see that there is particularly one area with many yellow points which means these area are poorly predicted by the fixed GWR model. If we toggle the mpsz_svy21 layer off and turn on open street map view, we can see that the area is Fernvale. There are also two areas with many orange points. From the open street map view, the areas are Damai, Oasis and Kadaloor in Punggol, and Sengkang South.

East

The following code chunk plots an interactive map within the East planning area.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="EAST REGION", ])+
  tm_polygons() +
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_sf_fixed1) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(10,15))
tmap_mode("plot")
```

In the east planning area, there are two areas with many yellow and orange points. If we toggle the mpsz_svy21 layer off and turn OpenStreetMap on, we can see that the poorly predicted areas are Tampines North, Tampines East, and around Pasir Ris Dr 12.

#### 9.2.2 Observed and Predicted Y

Visualization

The following code chunk plots the spatial point map of Observed and Predicted Y side by side using tmap_arrange function of tmap package.

```{r}
ymap <- tm_shape(mpsz_svy21)+
          tm_polygons() +
          tmap_options(check.and.fix = TRUE) +
        tm_shape(hdb_final_sf_fixed1) +  
          tm_dots(col = "y",
                  border.col = "gray60",
                  border.lwd = 1) +
        tm_layout(title="Observed Y")

yhatmap <- tm_shape(mpsz_svy21)+
              tm_polygons() +
              tmap_options(check.and.fix = TRUE) +
            tm_shape(hdb_final_sf_fixed1) +  
              tm_dots(col = "yhat",
                      border.col = "gray60",
                      border.lwd = 1) +
        tm_layout(title="Predicted Y")
        
tmap_arrange(ymap, yhatmap)
```

From the comparison, we can see that it looks the same in general which means that the prediction by the fixed GWR model is good.

Summary statistics

The following code chunk checks the summary statistics of the predicted value yhat using summary function

```{r}
summary(hdb_final_sf_fixed1$yhat)
```

From the summary statistics, the lowest predicted price for 4-room hdb flat is 236602 and the highest predicted price is 1077240. The average is 433767. This shows that there are only some 4 hdb flats with extremely high predicted price of over 1000000 as seen in the large difference between the third quartile and max resale price, 465594 and 1077240 respectively.

#### 9.2.3 Standardized Residual

Visualization

The following code chunk plots the spatial point map of standardized residual.

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons() +
  tmap_options(check.and.fix = TRUE) +
tm_shape(hdb_final_sf_fixed1) +  
  tm_dots(col = "Stud_residual",
          border.col = "gray60",
          border.lwd = 1) +
  tm_layout(legend.text.size = 0.8,
            title="Standardized Residual",
            title.size = 0.8)
```

From the residual map, we can see that majority of the points are yellow or light green which means that the standardized residual is between -5 to 0, or between 0 to 5. However, we cannot really determine the range because the bin of 5 is huge. Hence, we will need to see the summary statistic.

Summary statistics

The following code chunk checks the statistics of the standardized residual using summary function

```{r}
summary(hdb_final_sf_fixed1$Stud_residual)
```

From the summary statistics, the min is beyond -1 and the max is beyond 1. They are outliers because there is a huge difference from the min to 1st quartile, and 3rd quartile to max respectively. However, the mean and median residual value are low and almost near to 0, which shows that the prediction by the fixed GWR model in general are accurate. 